{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cQM07H9ua6JQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pickle as pkl\n",
        "import keras\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Input,Bidirectional,Dropout,LayerNormalization,Conv1D, BatchNormalization, \\\n",
        "    multiply, concatenate, Flatten, Activation, dot\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.optimizers import schedules\n",
        "from keras.layers.core import Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "import pydot as pyd\n",
        "from keras.utils.vis_utils import plot_model, model_to_dot\n",
        "# keras.utils.vis_utils.pydot = pyd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP9bFxfibwzm"
      },
      "source": [
        "# Data preprocessing part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xdTU6rGua6HQ"
      },
      "outputs": [],
      "source": [
        "def ACE_raw_feature4Delay(df, windowsize=10):\n",
        "    '''\n",
        "    df(DataFrame): the path of the processed sensor data\n",
        "    in our document, there is a csv file which has several traces, so the 'epoch' there means the number of this traces.\n",
        "    the 'flag' demonstrates if there has attack inside this trace(no matter before, now, or in the future in that trace), and the 'attack_status' demonstrate if the attack already happened before this time slot.\n",
        "    ACE_1, ACE_2, ACE_3 are the area control errors in different areas.\n",
        "    'simu_epoch' means the number of this epoch, and 'lastsample_index'is the index of the last sample in that window.\n",
        "    Return: the original data are cut into windows. And will return the cut windows(DataFrame).\n",
        "    '''\n",
        "    Raw_feature2 = []\n",
        "    simu_epoch = []\n",
        "    # the attack start index for each epoch, -1 for no attack\n",
        "    Attack_StartIndex = [] \n",
        "\n",
        "    # the index of the last sample for each segment\n",
        "    LastSample_Index = []\n",
        "    flag = []\n",
        "    df_noattack_f = df[df.flag == 0]\n",
        "    df_attack_f = df[df.flag >0]\n",
        "    label = []\n",
        "    ast = []\n",
        "\n",
        "    # no attack\n",
        "    for epoch in df_noattack_f.epoch.unique(): \n",
        "        df_epoch = df_noattack_f[df_noattack_f.epoch == epoch]\n",
        "        df_epoch.reset_index(inplace = True)\n",
        "        StartAttack_index = -1\n",
        "\n",
        "        # normalized sensors for feature extraction\n",
        "        df_n20 = np.array(df_epoch[['ACE2_n', 'ACE3_n', 'ACE1_n']])\n",
        "        for i in range(df_epoch.shape[0] - windowsize): \n",
        "            Raw_feature2.append(df_n20[i:i+windowsize].flatten())\n",
        "            simu_epoch.append(epoch)\n",
        "            flag.append(0)\n",
        "            ast.append(0)\n",
        "            Attack_StartIndex.append(StartAttack_index)\n",
        "            LastSample_Index.append(i+windowsize-1)\n",
        "\n",
        "    # has attack\n",
        "    for epoch in df_attack_f.epoch.unique(): \n",
        "        df_epoch = df_attack_f[df_attack_f.epoch == epoch]\n",
        "        df_epoch.reset_index(inplace = True)\n",
        "        StartAttack_index = df_epoch[df_epoch.attack_status >0].index[0]  \n",
        "\n",
        "        # normalized sensors for feature extraction\n",
        "        df_n20 = np.array(df_epoch[['ACE2_n', 'ACE3_n', 'ACE1_n']])\n",
        "        df_l21 = np.array(df_epoch[['attack_status']])\n",
        "        df_m21 = np.array(df_epoch[['flag']])\n",
        "        for i in range(df_epoch.shape[0] - windowsize):   \n",
        "            Raw_feature2.append(df_n20[i:i+windowsize].flatten())\n",
        "            simu_epoch.append(epoch)\n",
        "            flag.append(df_m21[i + windowsize-1][0])\n",
        "            ast.append(df_l21[i + windowsize-1][0])\n",
        "            Attack_StartIndex.append(StartAttack_index)\n",
        "            LastSample_Index.append(i+windowsize-1)\n",
        "            \n",
        "    # save landscape feature \n",
        "    df_raw = pd.DataFrame(Raw_feature2)\n",
        "    df_raw['simu_epoch'] = simu_epoch\n",
        "    df_raw['LastSample_Index'] = LastSample_Index \n",
        "    df_raw['Attack_StartIndex'] = Attack_StartIndex\n",
        "    df_raw['flag'] = flag\n",
        "    df_raw['ast'] = ast\n",
        "    return df_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KzMGagI6a6Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a29361-dd79-44b1-9aab-a77838a83512"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(491795,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# file for TD attack, csv document\n",
        "RawFileFold = '/data/delay_fixed.csv'\n",
        "df_raw = pd.read_csv(RawFileFold)\n",
        "\n",
        "# keep sensor of ACEs\n",
        "Scaler = MinMaxScaler()\n",
        "df_ACE = df_raw.copy()\n",
        "\n",
        "# normalize sensor data\n",
        "normal_data = np.array(df_ACE[['Ace_area_2', 'Ace_area_3', 'Ace_area_1', 'attack_status', 'flag']])\n",
        "df_ACE['ACE2_n'] = normal_data[:, 0]\n",
        "df_ACE['ACE3_n'] = normal_data[:, 1]\n",
        "df_ACE['ACE1_n'] = normal_data[:, 2]\n",
        "df_ACE['attack_status'] = normal_data[:, 3]\n",
        "df_ACE['flag'] = normal_data[:, 4]\n",
        "df_ACE['ACE1_n'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bv5ZdWRtbJ7r"
      },
      "outputs": [],
      "source": [
        "# split the whole data into normal epoches and abnormal epoches, perparing for data partation.\n",
        "df_TDA_noattack = df_ACE[df_ACE.flag == 0]\n",
        "df_TDA_attack1 = df_ACE[df_ACE.flag== 1]\n",
        "abnormal_epoch1 = list(df_TDA_attack1.iloc[:, -5].unique())\n",
        "normal_epoch = list(df_TDA_noattack.iloc[:, -5].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "q8Br1vzSs7So"
      },
      "outputs": [],
      "source": [
        "# split the data into train set, test set at 70%, 30%\n",
        "# 70% abnormal for train\n",
        "k1 = int(len(abnormal_epoch1)*0.7) \n",
        "\n",
        "# 70% normal for train\n",
        "k0 = int(len(normal_epoch)*0.7) \n",
        "\n",
        "# randomly select epoches and append them into the lists\n",
        "x_train_normal_epochs = []\n",
        "x_train_normal_epochs.extend(random.sample(normal_epoch, k = k0))\n",
        "x_train_abnormal_epochs = []\n",
        "x_train_abnormal_epochs.extend(random.sample(abnormal_epoch1, k = k1))\n",
        "x_test_normal_epochs = []\n",
        "for item in df_TDA_noattack.iloc[:, -5].unique():\n",
        "    if item not in x_train_normal_epochs:\n",
        "        x_test_normal_epochs.append(item)  \n",
        "x_test_abnormal_epochs = []\n",
        "for item in df_TDA_attack1.iloc[:, -5].unique():\n",
        "    if item not in x_train_abnormal_epochs:\n",
        "        x_test_abnormal_epochs.append(item)   \n",
        "\n",
        "# split the TDA dataset to 30% of testset and 70% of train set\n",
        "df_train_normal = df_TDA_noattack[df_TDA_noattack.iloc[:, -5].isin(x_train_normal_epochs)]\n",
        "df_test_normal = df_TDA_noattack[df_TDA_noattack.iloc[:, -5].isin(x_test_normal_epochs)]\n",
        "df_train_abnormal1 = df_TDA_attack1[df_TDA_attack1.iloc[:, -5].isin(x_train_abnormal_epochs)]\n",
        "df_test_abnormal1 = df_TDA_attack1[df_TDA_attack1.iloc[:, -5].isin(x_test_abnormal_epochs)]\n",
        "trainable1 = df_train_normal.append(df_train_abnormal1)\n",
        "testable1 = df_test_normal.append(df_test_abnormal1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wXmoOWU2a6Dt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e7f2b2-d6f1-4543-c157-26b432331634"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1290000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# data perparing\n",
        "# file for Stealthy FDI attack\n",
        "RawFileFold = '/data/stealthy_All.csv'\n",
        "df_raw = pd.read_csv(RawFileFold)\n",
        "\n",
        "# keep sensor of ACEs\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "Scaler = MinMaxScaler()\n",
        "df_ACE = df_raw.copy()\n",
        "\n",
        "# normalize sensor data\n",
        "normal_data = np.array(df_ACE[['Ace_area_2', 'Ace_area_3', 'Ace_area_1', 'attack_status', 'flag']])\n",
        "df_ACE['ACE2_n'] = normal_data[:, 0]\n",
        "df_ACE['ACE3_n'] = normal_data[:, 1]\n",
        "df_ACE['ACE1_n'] = normal_data[:, 2]\n",
        "df_ACE['attack_status'] = normal_data[:, 3]\n",
        "df_ACE['flag'] = normal_data[:, 4]\n",
        "df_ACE['ACE1_n'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WgdleONpbUed"
      },
      "outputs": [],
      "source": [
        "# In our project, the label for TDA is 1 and the label for FDIA is 2, for no attack is 0\n",
        "df_ACE.epoch += 10000\n",
        "df_ACE.flag *= 2\n",
        "b = []\n",
        "for a in df_ACE.attack_status:\n",
        "    b.append(2 if a>0 else 0)\n",
        "\n",
        "# make the FDI attacks have different label\n",
        "df_ACE.attack_status = pd.DataFrame(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2MqcL6sya6CD"
      },
      "outputs": [],
      "source": [
        "# split the whole data into normal epoches and abnormal epoches, perparing for data partation.\n",
        "df_FDIA_noattack = df_ACE[df_ACE.flag == 0]\n",
        "df_FDIA_attack1 = df_ACE[df_ACE.flag == 2]\n",
        "abnormal_epoch1 = list(df_FDIA_attack1.epoch.unique())\n",
        "normal_epoch = list(df_FDIA_noattack.epoch.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G5wMaATFa5_r"
      },
      "outputs": [],
      "source": [
        "# split the data into train set, test set at 70%, 30%\n",
        "# 70% abnormal for train\n",
        "k1 = int(len(abnormal_epoch1)*0.7) \n",
        "\n",
        "# 70% normal for train\n",
        "k0 = int(len(normal_epoch)*0.7) \n",
        "\n",
        "# randomly select epoches and append them into the lists\n",
        "x_train_normal_epochs = []\n",
        "x_train_normal_epochs.extend(random.sample(normal_epoch, k = k0))\n",
        "x_train_abnormal_epochs = []\n",
        "x_train_abnormal_epochs.extend(random.sample(abnormal_epoch1, k = k1))\n",
        "x_test_normal_epochs = []\n",
        "for item in df_FDIA_noattack.iloc[:, -5].unique():\n",
        "    if item not in x_train_normal_epochs:\n",
        "        x_test_normal_epochs.append(item)  \n",
        "x_test_abnormal_epochs = []\n",
        "for item in df_FDIA_attack1.iloc[:, -5].unique():\n",
        "    if item not in x_train_abnormal_epochs:\n",
        "        x_test_abnormal_epochs.append(item)   \n",
        "\n",
        "# split the FDIA dataset to 30% of testset and 70% of train set\n",
        "df_train_normal = df_FDIA_noattack[df_FDIA_noattack.iloc[:, -5].isin(x_train_normal_epochs)]\n",
        "df_test_normal = df_FDIA_noattack[df_FDIA_noattack.iloc[:, -5].isin(x_test_normal_epochs)]\n",
        "df_train_abnormal1 = df_FDIA_attack1[df_FDIA_attack1.iloc[:, -5].isin(x_train_abnormal_epochs)]\n",
        "df_test_abnormal1 = df_FDIA_attack1[df_FDIA_attack1.iloc[:, -5].isin(x_test_abnormal_epochs)]\n",
        "trainable2 = df_train_normal.append(df_train_abnormal1)\n",
        "testable2 = df_test_normal.append(df_test_abnormal1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3EJAdqsZa58O"
      },
      "outputs": [],
      "source": [
        "# concatenate the data of FDIA and TDA\n",
        "testable = pd.concat([testable1, testable2])\n",
        "trainable = pd.concat([trainable1, trainable2])\n",
        "dftest = ACE_raw_feature4Delay(testable)\n",
        "dftrain = ACE_raw_feature4Delay(trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ShKMk74Na558"
      },
      "outputs": [],
      "source": [
        "# the processing of minmax scaler\n",
        "Scaler = MinMaxScaler()\n",
        "a = Scaler.fit(dftrain.iloc[:, :30].values.reshape(dftrain.shape[0]*30, 1))\n",
        "df1 = a.transform(dftrain.iloc[:, :30].values.reshape(dftrain.shape[0]*30, 1))\n",
        "dftrain = pd.concat(objs = [pd.DataFrame(df1.reshape(dftrain.shape[0], 30)),dftrain.iloc[:, 30:]],axis = 1,ignore_index = True)\n",
        "df1 = a.transform(dftest.iloc[:, :30].values.reshape(dftest.shape[0]*30, 1))\n",
        "dftest = pd.concat(objs = [pd.DataFrame(df1.reshape(dftest.shape[0], 30)),dftest.iloc[:, 30:]],axis = 1,ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HArF3yeua51s"
      },
      "outputs": [],
      "source": [
        "# Only Xtrain will be used for the Autoencoder training\n",
        "Xtrain = pd.DataFrame(dftrain).iloc[:, :30].values.reshape(dftrain.shape[0], 10, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk9jvJvjbp96"
      },
      "source": [
        "# AutoEncoder training part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nhBh8opVa5yy"
      },
      "outputs": [],
      "source": [
        "ENC_PROJECTION_DIM = 140\n",
        "ENC_NUM_HEADS = 3\n",
        "ENC_LAYERS = 3\n",
        "NUM_PATCHES = 2\n",
        "DEC_LAYERS = 2\n",
        "DEC_NUM_HEADS = 3\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "DEC_PROJECTION_DIM = 140\n",
        "ENC_TRANSFORMER_UNITS = 140\n",
        "MLPHL = [140]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wdP5dHdvs7Sv"
      },
      "outputs": [],
      "source": [
        "step = tf.Variable(0, trainable=False)\n",
        "boundaries = [100000, 150000]\n",
        "values = [1e-3, 3e-3, 2e-3]\n",
        "learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries, values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kMXHs-X_a5tT"
      },
      "outputs": [],
      "source": [
        "def mlp(x, dropout_rate, hidden_units):\n",
        "    '''\n",
        "    The MLP layer, included Dense layer and dropout layer\n",
        "    '''\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation = tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YyypASKta5rk"
      },
      "outputs": [],
      "source": [
        "def create_encoder(num_heads = ENC_NUM_HEADS, num_layers = ENC_LAYERS):\n",
        "    '''\n",
        "    The encoder of the autoencoder\n",
        "    this part will extract the high-dimensional representations out\n",
        "    '''\n",
        "    inputs = layers.Input((None, ENC_PROJECTION_DIM))\n",
        "    x = inputs\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon = LAYER_NORM_EPS)(x)\n",
        "\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads = num_heads, key_dim = ENC_PROJECTION_DIM, dropout = 0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon = LAYER_NORM_EPS)(x2)\n",
        "\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units = MLPHL, dropout_rate = 0.1)\n",
        "\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    outputs = layers.LayerNormalization(epsilon = LAYER_NORM_EPS)(x)\n",
        "    return keras.Model(inputs, outputs, name = \"ae_encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "J_UuRCKlayGz"
      },
      "outputs": [],
      "source": [
        "def create_decoder(\n",
        "    num_layers = DEC_LAYERS, num_heads = DEC_NUM_HEADS\n",
        "):\n",
        "    '''\n",
        "    The decoder of the autoencoder\n",
        "    it will reconstruct the original data by using the extracted representations\n",
        "    '''\n",
        "    inputs = layers.Input((NUM_PATCHES, ENC_PROJECTION_DIM))\n",
        "    x = layers.Dense(DEC_PROJECTION_DIM)(inputs)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n",
        "\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads = num_heads, key_dim = DEC_PROJECTION_DIM, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon = LAYER_NORM_EPS)(x2)\n",
        "\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units = MLPHL, dropout_rate = 0.1)\n",
        "\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    x = layers.LayerNormalization(epsilon = 1e-6)(x)\n",
        "    outputs = layers.Dense(15)(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs, name = \"ae_decoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YvRv3TXkcNQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609d6be7-d8ab-4c57-bead-7ee045df85a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 10, 3)]           0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 2, 15)             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2, 140)            2240      \n",
            "                                                                 \n",
            " ae_encoder (Functional)     (None, None, 140)         770980    \n",
            "                                                                 \n",
            " ae_decoder (Functional)     (None, 2, 15)             535935    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 10, 3)             0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,309,155\n",
            "Trainable params: 1,309,155\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder = create_encoder()\n",
        "decoder = create_decoder()\n",
        "\n",
        "# the autoencoder\n",
        "inputs = layers.Input(shape=(10, 3))\n",
        "# the reshape layer is totally used for the event mechanism, which will get better performance.\n",
        "x=layers.Reshape((2, 15))(inputs)\n",
        "embeddings = layers.Dense(140)(x)\n",
        "encoder_outputs = encoder(embeddings)\n",
        "decoder_outputs = decoder(encoder_outputs)\n",
        "decoder_outputs = layers.Reshape((10, 3))(decoder_outputs)\n",
        "model = Model(inputs = inputs, outputs = decoder_outputs)\n",
        "opt = tfa.optimizers.AdamW(learning_rate = learning_rate_fn(step), weight_decay = 1e-4)\n",
        "model.compile(loss = 'mae', optimizer = opt)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nHdtYVk6s7Sy"
      },
      "outputs": [],
      "source": [
        "modelpath = '/model/AE5event'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jk6rYnG8cNPF",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41cd022-69c4-4cf1-873c-6a2eabbc40c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "936/937 [============================>.] - ETA: 0s - loss: 0.0443\n",
            "Epoch 1: val_loss improved from inf to 0.02410, saving model to /content/drive/MyDrive/ADSC/MAE/AE5event\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r937/937 [==============================] - 49s 44ms/step - loss: 0.0442 - val_loss: 0.0241\n",
            "Epoch 2/3\n",
            "936/937 [============================>.] - ETA: 0s - loss: 0.0195\n",
            "Epoch 2: val_loss improved from 0.02410 to 0.02154, saving model to /content/drive/MyDrive/ADSC/MAE/AE5event\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r937/937 [==============================] - 41s 43ms/step - loss: 0.0195 - val_loss: 0.0215\n",
            "Epoch 3/3\n",
            "937/937 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 3: val_loss improved from 0.02154 to 0.01861, saving model to /content/drive/MyDrive/ADSC/MAE/AE5event\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r937/937 [==============================] - 41s 43ms/step - loss: 0.0145 - val_loss: 0.0186\n"
          ]
        }
      ],
      "source": [
        "# for training, each epoch consume around 100s, usually 60 epoch is enough.\n",
        "epc = 3\n",
        "es = keras.callbacks.EarlyStopping(\n",
        "monitor = 'val_loss',\n",
        "min_delta = 0, \n",
        "patience = 35)\n",
        "history = model.fit(Xtrain,Xtrain, validation_split = 0.15, \n",
        "                    epochs = epc, verbose = 1, callbacks = [ModelCheckpoint(filepath = modelpath, monitor = \"val_loss\", verbose = 1, save_best_only = True), es], \n",
        "                    batch_size = 1024,shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6pNfymXctWl"
      },
      "source": [
        "# Evaluation part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KueDERD6s7S0"
      },
      "outputs": [],
      "source": [
        "def split(normalnum, FDIAnum, TDAnum, dftrain, dftest):\n",
        "    '''\n",
        "    extract the dataset for classifier training(Only for classifier)\n",
        "    normalnum(int): number of normal data for clasifier training\n",
        "    FDIAnum(int): number of FDIA data for clasifier training\n",
        "    TDAnum(int): number of TDA data for clasifier training\n",
        "    dftrain(DataFrame): training dataset\n",
        "    dftest(DataFrame): testing dataset\n",
        "    return: the splited data(np.array)\n",
        "    '''\n",
        "    dftrain=np.array(dftrain)\n",
        "    df_TDA_noattack = pd.DataFrame(dftrain[dftrain[:, -1] == 0])\n",
        "    df_TDA_attack1 = pd.DataFrame(dftrain[dftrain[:, -1] == 1])\n",
        "    df_TDA_attack2 = pd.DataFrame(dftrain[dftrain[:, -1] == 2])\n",
        "\n",
        "    #select some labeled data for downstream classifier training\n",
        "    train_normal = df_TDA_noattack.sample(n = normalnum)\n",
        "    test_normal = df_TDA_noattack[~df_TDA_noattack.index.isin(train_normal.index)]\n",
        "    train_abnormal1 = df_TDA_attack1.sample(n = FDIAnum)\n",
        "    test_abnormal1 = df_TDA_attack1[~df_TDA_attack1.index.isin(train_abnormal1.index)]\n",
        "    train_abnormal2 = df_TDA_attack2.sample(n = TDAnum)\n",
        "    test_abnormal2 = df_TDA_attack2[~df_TDA_attack2.index.isin(train_abnormal2.index)]\n",
        "    train1 = pd.concat([train_normal, train_abnormal1, train_abnormal2])\n",
        "    test1 = pd.concat([test_normal, test_abnormal1, test_abnormal2])\n",
        "    dftest = pd.DataFrame(dftest)\n",
        "    Xtrain = train1.iloc[:, :30].values.reshape(train1.shape[0], 10, 3)\n",
        "    Ytrain = train1.iloc[:, -1]\n",
        "    Xtest = dftest.iloc[:, :30].values.reshape(dftest.shape[0], 10, 3)\n",
        "    Ytest = dftest.iloc[:, -1]\n",
        "    return Xtrain, Xtest, Ytrain, Ytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "40-Rf1Q7s7S0"
      },
      "outputs": [],
      "source": [
        "def GMMsplit(GMMnornum, GMMFDIAnum, GMMTDAnum, dftrain):\n",
        "    '''\n",
        "    extract the dataset for GMM training(Only for GMM)\n",
        "    GMMnornum(int): number of normal data for GMM training\n",
        "    GMMFDIAnum(int): number of FDIA data for GMM training\n",
        "    GMMTDAnum(int): number of TDA data for GMM training\n",
        "    dftrain(DataFrame): training dataset\n",
        "    return: the splited data(np.array)\n",
        "    '''\n",
        "    dftrain=np.array(dftrain)\n",
        "    df_TDA_noattack = pd.DataFrame(dftrain[dftrain[:, -1] == 0])\n",
        "    df_TDA_attack1 = pd.DataFrame(dftrain[dftrain[:, -1] == 1])\n",
        "    df_TDA_attack2 = pd.DataFrame(dftrain[dftrain[:, -1] == 2])\n",
        "\n",
        "    # select some unlabeled data for GMM training, will improve the accuracy by about 1%\n",
        "    train_normal = df_TDA_noattack.sample(n = GMMnornum)\n",
        "    test_normal = df_TDA_noattack[~df_TDA_noattack.index.isin(train_normal.index)]\n",
        "    train_abnormal11 = df_TDA_attack1.sample(n = GMMFDIAnum)\n",
        "    test_abnormal11 = df_TDA_attack1[~df_TDA_attack1.index.isin(train_abnormal11.index)]\n",
        "    train_abnormal21 = df_TDA_attack2.sample(n = GMMTDAnum)\n",
        "    test_abnormal21 = df_TDA_attack2[~df_TDA_attack2.index.isin(train_abnormal21.index)]\n",
        "    train2 = pd.concat([train_normal, train_abnormal11 ,train_abnormal21])\n",
        "    XtrainGMM = train2.iloc[:, :30].values.reshape(train2.shape[0], 10, 3)\n",
        "    return XtrainGMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "C7-QUnE5s7S1"
      },
      "outputs": [],
      "source": [
        "normalnum = 200\n",
        "FDIAnum = 100\n",
        "TDAnum = 100\n",
        "GMMnornum = 40000\n",
        "GMMFDIAnum = 40000\n",
        "GMMTDAnum = 40000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lSQ47rGKs7S1"
      },
      "outputs": [],
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = split(normalnum, FDIAnum, TDAnum, dftrain, dftest)\n",
        "XtrainGMM = GMMsplit(GMMnornum, GMMFDIAnum, GMMTDAnum, dftrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xtGuOscfdJRK"
      },
      "outputs": [],
      "source": [
        "def acc(Y_test, Ypred):\n",
        "    '''\n",
        "    code for Precision and Recall for different attacks' calculation\n",
        "    Y_test(np_array): The ground true label for the testset.\n",
        "    Ypred(np_array): The predicted label by the downstream classifier.\n",
        "    Return: the f1-score for different attacks(float).\n",
        "    '''\n",
        "    TN0 = 0\n",
        "    TP0 = 0\n",
        "    FP0 = 0\n",
        "    FN0 = 0\n",
        "    TN1 = 0\n",
        "    TP1 = 0\n",
        "    FP1 = 0\n",
        "    FN1 = 0\n",
        "    TN2 = 0\n",
        "    TP2 = 0\n",
        "    FP2 = 0\n",
        "    FN2 = 0\n",
        "    for i in range(Ypred.shape[0]):\n",
        "        if Y_test[i] == 1:\n",
        "            if Ypred[i] == 1:\n",
        "                TP1 += 1\n",
        "            else:FN1 += 1\n",
        "        elif Y_test[i] != 1:\n",
        "            if Ypred[i] != 1:\n",
        "                TN1 += 1\n",
        "            else:FP1 += 1\n",
        "    for i in range(Ypred.shape[0]):\n",
        "        if Y_test[i] == 2:\n",
        "            if Ypred[i] == 2:\n",
        "                TP2 += 1\n",
        "            else:FN2 += 1\n",
        "        elif Y_test[i] != 2:\n",
        "            if Ypred[i] != 2:\n",
        "                TN2 += 1\n",
        "            else:FP2 += 1\n",
        "    for i in range(Ypred.shape[0]):\n",
        "        if Y_test[i] == 0:\n",
        "            if Ypred[i] == 0:\n",
        "                TP0 += 1\n",
        "            else:FN0 += 1\n",
        "        elif Y_test[i] != 0:\n",
        "            if Ypred[i] != 0:\n",
        "                TN0 += 1\n",
        "            else:FP0 += 1\n",
        "    a = 'for 0 precision/recall/f1:'\n",
        "    b = 'for Delay attack precision/recall/f1:'\n",
        "    c = 'for FDI precision/recall/f1:'\n",
        "    print(a, TP0/(FP0+TP0), TP0/(FN0+TP0), (TP0/(FP0+TP0)+TP0/(FN0+TP0))/2, b, TP1/(FP1+TP1), TP1/(FN1+TP1), (TP1/(FP1+TP1)+TP1/(FN1+TP1))/2, c, TP2/(FP2+TP2), TP2/(FN2+TP2), (TP2/(FP2+TP2)+TP2/(FN2+TP2))/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqtKkIpBs7S2",
        "outputId": "e9d93efd-9849-4fba-e0ec-1c12f5524e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 10, 3)]           0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 2, 15)             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2, 140)            2240      \n",
            "                                                                 \n",
            " ae_encoder (Functional)     (None, None, 140)         770980    \n",
            "                                                                 \n",
            " ae_decoder (Functional)     (None, 2, 15)             535935    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 10, 3)             0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,309,155\n",
            "Trainable params: 1,309,155\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "<keras.engine.functional.Functional object at 0x7f6ed40d7910>\n"
          ]
        }
      ],
      "source": [
        "#representation extraction\n",
        "model = load_model(modelpath)\n",
        "model.summary()\n",
        "\n",
        "#We mainly need the first to the forth layers for the representation extractor.\n",
        "reshape = model.layers[1]\n",
        "dense = model.layers[2]\n",
        "\n",
        "# Extract the encoder.\n",
        "encoder = model.layers[3]\n",
        "print(encoder)\n",
        "\n",
        "# Pack as a model.\n",
        "patches = layers.Input(shape=(10, 3))\n",
        "x = reshape(patches)\n",
        "patch_embeddings = dense(x)\n",
        "encoder_outputs = encoder(patch_embeddings)\n",
        "\n",
        "# The outputs are the extracted representations \n",
        "downstream_model = Model(inputs = patches, outputs = encoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18SkXadrs7S2",
        "outputId": "06bbec81-ccc8-4e78-8a6f-0af02c93639a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=1000, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# train the GMM and the downstream classifier\n",
        "pat=layers.Input(shape=(2,140))\n",
        "m = model.layers[4]\n",
        "n = model.layers[5]\n",
        "x1 = m(pat)\n",
        "x2 = n(x1)\n",
        "decoder = Model(inputs=pat, outputs=x2)\n",
        "\n",
        "# and the rest of layers are used for reconstruction error calculation, which is used for the GMM training.\n",
        "Xpredtra = model.predict(XtrainGMM)\n",
        "\n",
        "# the reconstruction error\n",
        "error_train = XtrainGMM-Xpredtra\n",
        "error_train = error_train.reshape(error_train.shape[0],30)\n",
        "gm = GaussianMixture(n_components=27, random_state=0).fit(error_train)\n",
        "Xpredinp = model.predict(Xtrain)\n",
        "error_inp = Xtrain-Xpredinp\n",
        "error_inp = error_inp.reshape(error_inp.shape[0],30)\n",
        "\n",
        "# the error distribution\n",
        "trainfea = gm.predict(error_inp)\n",
        "RFinp = downstream_model.predict(Xtrain)\n",
        "RFinp = RFinp.reshape(RFinp.shape[0], RFinp.shape[1]*RFinp.shape[2])\n",
        "trainfea = trainfea.reshape(trainfea.shape[0],1)\n",
        "\n",
        "#New representations.\n",
        "RFinp1 = np.concatenate([RFinp, trainfea],axis=1)\n",
        "clf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
        "clf.fit(RFinp1, Ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAhZsHc1s7S2",
        "outputId": "7cdda8b9-d1cb-4913-a1eb-5929bbfb93f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0004329291351073239\n",
            "for 0 precision/recall/f1: 0.9369936679392867 0.9836684568433015 0.9603310623912942 for Delay attack precision/recall/f1: 0.8592912100722779 0.6324649917627677 0.7458781009175228 for FDI precision/recall/f1: 0.9799215750682625 0.9707144455064978 0.9753180102873802\n"
          ]
        }
      ],
      "source": [
        "# computation overhead measure and the f1-score calculating\n",
        "start = time.perf_counter()\n",
        "RFpred = downstream_model.predict(Xtest)\n",
        "Xpredtes = decoder.predict(RFpred)\n",
        "error_test = Xtest-Xpredtes\n",
        "error_test = error_test.reshape(error_test.shape[0],30)\n",
        "testfea = gm.predict(error_test)\n",
        "testfea = testfea.reshape(testfea.shape[0],1)\n",
        "RFpred = RFpred.reshape(RFpred.shape[0], RFpred.shape[1]*RFpred.shape[2])\n",
        "RFpred1 = np.concatenate([RFpred, testfea],axis=1)\n",
        "Ypred = clf.predict(RFpred1)\n",
        "end = time.perf_counter()\n",
        "print ((end-start)/Xtest.shape[0])\n",
        "Y_test = np.array(Ytest)\n",
        "acc(Y_test, Ypred)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PowerBERT1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}